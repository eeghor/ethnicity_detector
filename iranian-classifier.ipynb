{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import classification_report\n",
    "from pprint import pprint\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IranianClassifier:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.data = pd.read_csv('/Users/ik/Data/names/training-iranian.csv')\n",
    "        \n",
    "    def _into_words(self):\n",
    "        \"\"\"\n",
    "        split full names into words and add these to the dataset with the corresponding labels\n",
    "        \"\"\"\n",
    "        _ir = set()\n",
    "        _nonir = set()\n",
    "        \n",
    "        for row in self.data.iterrows():\n",
    "            _ir.add(row[1]['full_name'].split()[0]) if row[1].is_iranian == 1 else _nonir.update(set(row[1]['full_name'].split()))\n",
    "        \n",
    "        # iranian name words not found in the non-iranian name words\n",
    "        self.data = pd.concat([self.data,\n",
    "                               pd.DataFrame({'full_name': list(_ir - _nonir), 'is_iranian': 1}),\n",
    "                                  pd.DataFrame({'full_name': list(_nonir - _ir), 'is_iranian': 0})]).sample(frac=1.)\n",
    "        \n",
    "        \n",
    "        return self\n",
    "\n",
    "class Selector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    select a columns from a data frame\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, col_name):\n",
    "        self.col_name = col_name\n",
    "    \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, x):\n",
    "        return x[self.col_name]\n",
    "    \n",
    "class LastName(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Extract features from a Series of first names\n",
    "    \"\"\"\n",
    "    def _ngram(self, s, n):\n",
    "        \"\"\"\n",
    "        extract n-gram counts from string s\n",
    "        \"\"\"\n",
    "        if len(s) < n:\n",
    "            return {}\n",
    "        \n",
    "        f = defaultdict(int)\n",
    "        \n",
    "        for i, c in enumerate(s, 1):\n",
    "            if i + n <= len(s):\n",
    "                ngram = s[i-1: i+n]            \n",
    "                if ngram in f:\n",
    "                    f[ngram] += 1 \n",
    "                else:\n",
    "                    f[ngram] = 1\n",
    "        return f\n",
    "    \n",
    "    def _last_n(self, s, n):\n",
    "        \"\"\"\n",
    "        extract ending (last n letters) from s\n",
    "        \"\"\"\n",
    "        if len(s[-n:]) == n:\n",
    "            return s[-n:]\n",
    "                    \n",
    "    def _lastname_features(self, s):\n",
    "        \"\"\"\n",
    "        take a string presumably first name and extract features as a dictionary\n",
    "        \"\"\"\n",
    "        \n",
    "        # keep features here\n",
    "        fn_feats = defaultdict()\n",
    "        \n",
    "        # if s not even a string, no features\n",
    "        if not isinstance(s, str):\n",
    "            return fn_feats\n",
    "        \n",
    "        for p in (set(punctuation) - {\"'\",\"-\"}):\n",
    "            s = s.lower().replace(p,'')\n",
    "        \n",
    "        word1 = s.split()[0]\n",
    "        \n",
    "        # if s is one letter (maybe an initial), no features\n",
    "        if len(s) < 2:\n",
    "            return fn_feats\n",
    "        \n",
    "        # prefix to name features\n",
    "        pref = 'lname_'\n",
    "        # how many words in first name?\n",
    "        fn_feats['num_' + pref + 'nwords'] = len(s.split())\n",
    "        # first letter of the first word\n",
    "        fn_feats['nom_ + pref + 'first_letter'] = s[0]\n",
    "        # length of first word\n",
    "        fn_feats['num_' + pref + 'len'] = len(s.split()[0])\n",
    "        # last letter of the first word\n",
    "        fn_feats['nom_ + pref + 'last_letter'] = s.split()[0][-1]\n",
    "        # words themselves\n",
    "        for j, w in enumerate(s.split(), 1):\n",
    "            fn_feats['num_' + pref + 'word_' + str(j)] = w\n",
    "        # any hyphens?\n",
    "        if '-' in s:\n",
    "            fn_feats['bin_' +pref + 'hyphen'] = 1\n",
    "        # any apostrophs?\n",
    "        if \"'\" in s:\n",
    "            fn_feats['bin_' +pref + 'apostr'] = 1\n",
    "        # letter counts (first word)\n",
    "        for c in Counter(word1).items():\n",
    "            fn_feats['num_' + pref + 'letter_' + c[0]] = c[1]\n",
    "        \n",
    "        # ending - last n letters\n",
    "        for n in range(1,5):\n",
    "            ending = self._last_n(word1, n)\n",
    "            if ending:\n",
    "                fn_feats['bin_' + pref + str(n) + '_last_lettes_' + ending] = 1 \n",
    "            \n",
    "        for n in range(1,5):\n",
    "            for gr, v in self._ngram(word1, n).items():\n",
    "                fn_feats['num_' + pref + str(n)+ '_gram_' + gr] = v\n",
    "        \n",
    "        return fn_feats\n",
    "        \n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, x):\n",
    "        \n",
    "        return pd.DataFrame.from_dict({t[0]: t[1] for t in \n",
    "                                       zip(x.index.values, x.apply(lambda x: self._lastname_features(x)))}, \n",
    "                                          orient='index').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    ic = IranianClassifier()._into_words()\n",
    "    \n",
    "    # split into the training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(ic.data.drop(['is_iranian'], axis=1), \n",
    "                                                        ic.data['is_iranian'],  test_size=0.2, \n",
    "                                                        random_state=391, stratify=ic.data['is_iranian'])\n",
    "    \n",
    "    # activate last name feature extractor\n",
    "    ln = LastName()\n",
    "    \n",
    "    w = ln.fit_transform(Selector('full_name').fit_transform(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
