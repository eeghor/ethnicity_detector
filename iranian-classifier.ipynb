{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from pprint import pprint\n",
    "from string import punctuation\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IranianNames:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.data = pd.read_csv('/Users/ik/Data/names/training-iranian.csv')\n",
    "        \n",
    "    def _into_words(self):\n",
    "        \"\"\"\n",
    "        split full names into words and add these to the dataset with the corresponding labels\n",
    "        \"\"\"\n",
    "        _ir = set()\n",
    "        _nonir = set()\n",
    "        \n",
    "        for row in self.data.iterrows():\n",
    "            _ir.add(row[1]['full_name'].split()[0]) if row[1].is_iranian == 1 else _nonir.update(set(row[1]['full_name'].split()))\n",
    "        \n",
    "        # iranian name words not found in the non-iranian name words\n",
    "        self.data = pd.concat([self.data,\n",
    "                               pd.DataFrame({'full_name': list(_ir - _nonir), 'is_iranian': 1}),\n",
    "                                  pd.DataFrame({'full_name': list(_nonir - _ir), 'is_iranian': 0})]).reset_index(drop=True).sample(frac=1.)\n",
    "        \n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Selector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    select a columns from a data frame\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, col_name):\n",
    "        self.col_name = col_name\n",
    "    \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, x):\n",
    "        return x.loc[:, self.col_name]\n",
    "    \n",
    "class LastName(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Extract features from a Series of first names\n",
    "    \"\"\"\n",
    "    def _ngram(self, s, n):\n",
    "        \"\"\"\n",
    "        extract n-gram counts from string s\n",
    "        \"\"\"\n",
    "        \n",
    "        f = defaultdict(int)\n",
    "        \n",
    "        if len(s) < n:\n",
    "            return f   \n",
    "        \n",
    "        for i, c in enumerate(s, 1):\n",
    "            if i + n <= len(s):\n",
    "                ngram = s[i-1: i+n]            \n",
    "                if ngram in f:\n",
    "                    f[ngram] += 1 \n",
    "                else:\n",
    "                    f[ngram] = 1\n",
    "        return f\n",
    "    \n",
    "    def _last_n(self, s, n):\n",
    "        \"\"\"\n",
    "        extract ending (last n letters) from s\n",
    "        \"\"\"\n",
    "        if len(s[-n:]) == n:\n",
    "            return s[-n:]\n",
    "                    \n",
    "    def _lastname_features(self, s):\n",
    "        \"\"\"\n",
    "        take a string presumably first name and extract features as a dictionary\n",
    "        \"\"\"\n",
    "        \n",
    "        # keep features here\n",
    "        fn_feats = defaultdict()\n",
    "        \n",
    "        # if s not even a string, no features\n",
    "        if not isinstance(s, str):\n",
    "            print(f'{s} not a string')\n",
    "            return fn_feats\n",
    "        \n",
    "        for p in (set(punctuation) - {\"'\",\"-\"}):\n",
    "            s = s.lower().replace(p,'')\n",
    "        \n",
    "        word1 = s.split()[0]\n",
    "        \n",
    "        # prefix to name features\n",
    "        pref = 'lname_'\n",
    "        # how many words in first name?\n",
    "        fn_feats['num_' + pref + 'nwords'] = len(s.split()) if len(s) > 1 else 0\n",
    "        # first letter of the first word\n",
    "        fn_feats['nom_' + pref + 'first_letter'] = s[0]\n",
    "        # length of first word\n",
    "        fn_feats['num_' + pref + 'len'] = len(s.split()[0]) if len(s) > 1 else 0\n",
    "        # last letter of the first word\n",
    "        fn_feats['nom_' + pref + 'last_letter'] = s.split()[0][-1] if len(s) > 1 else 0\n",
    "        # words themselves\n",
    "        if len(s) > 1:\n",
    "            for j, w in enumerate(s.split(), 1):\n",
    "                fn_feats['nom_' + pref + 'word_' + str(j)] = w \n",
    "        # any hyphens?\n",
    "        if len(s) > 1:\n",
    "            if '-' in s:\n",
    "                fn_feats['bin_' + pref + 'hyphen'] = 1\n",
    "            # any apostrophs?\n",
    "            if \"'\" in s:\n",
    "                fn_feats['bin_' + pref + 'apostr'] = 1\n",
    "            # letter counts (first word)\n",
    "            for c in Counter(word1).items():\n",
    "                fn_feats['num_' + pref + 'letter_' + c[0]] = c[1]\n",
    "        \n",
    "        # ending - last n letters\n",
    "        if len(s) > 1:\n",
    "            for n in range(1,5):\n",
    "                ending = self._last_n(word1, n)\n",
    "                if ending:\n",
    "                    fn_feats['bin_' + pref + str(n) + '_last_lettes_' + ending] = 1 \n",
    "            \n",
    "            for n in range(1,5):\n",
    "                for gr, v in self._ngram(word1, n).items():\n",
    "                    fn_feats['num_' + pref + str(n)+ '_gram_' + gr] = v\n",
    "        \n",
    "        return fn_feats\n",
    "        \n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, x):\n",
    "        \n",
    "        x = x.reset_index(drop=True)\n",
    "        \n",
    "        print('transform received ', x.shape)\n",
    "        print('x.index.values = ', len(x.index.values))\n",
    "        print('unique in index=',len(x.index.unique()))\n",
    "        \n",
    "        res =  pd.DataFrame.from_dict({t[0]: t[1] for t in \n",
    "                                       zip(x.index.values, x.apply(self._lastname_features))}, \n",
    "                                          orient='index').fillna(0)\n",
    "        print(res.shape)\n",
    "        # create dummy variables for nominal features\n",
    "        return pd.get_dummies(res, columns=[c for c in res.columns if c.startswith('nom_')]).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape= (4620, 1)  Y_train shape= (4620,)\n",
      "transform received  (4620,)\n",
      "x.index.values =  4620\n",
      "unique in index= 4620\n",
      "(4620, 16643)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    ic = IranianNames()._into_words()\n",
    "    \n",
    "\n",
    "    # split into the training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(ic.data.drop(['is_iranian'], axis=1), \n",
    "                                                        ic.data['is_iranian'],  test_size=0.2, \n",
    "                                                        random_state=391, stratify=ic.data['is_iranian'])\n",
    "    \n",
    "\n",
    "    pipe = Pipeline([('fe_union', FeatureUnion([('ff', Pipeline([('select_last_name', Selector('full_name')),\n",
    "                                            ('last_name_features', LastName())]))])),\n",
    "                    ('clf', SGDClassifier())])\n",
    "    print('X_train shape=', X_train.shape, ' Y_train shape=',y_train.shape)\n",
    "    pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transform received  (1156,)\n",
      "x.index.values =  1156\n",
      "unique in index= 1156\n",
      "(1156, 7211)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 8806 features per sample; expecting 21438",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-77a5286020a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/utils/metaestimators.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;31m# update the docstring of the returned function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mupdate_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mif_delegate_has_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelegate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'_final_estimator'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \"\"\"\n\u001b[0;32m--> 324\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m             raise ValueError(\"X has %d features per sample; expecting %d\"\n\u001b[0;32m--> 305\u001b[0;31m                              % (X.shape[1], n_features))\n\u001b[0m\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         scores = safe_sparse_dot(X, self.coef_.T,\n",
      "\u001b[0;31mValueError\u001b[0m: X has 8806 features per sample; expecting 21438"
     ]
    }
   ],
   "source": [
    "pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
