{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import enchant\n",
    "import json\n",
    "import statistics\n",
    "from unidecode import unidecode\n",
    "from collections import Counter, defaultdict\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChineseFinder(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.ench_dic = enchant.Dict(\"en_US\")  # english spellcheck\n",
    "        # chinese names and surnames\n",
    "        self.chinese_names = set(chain.from_iterable([unidecode(part).replace(\"-\",\" \").split() \n",
    "                                    for full_name in pd.read_csv(\"names_cn.txt\", header=None)[0].tolist() \n",
    "                                                      for part in full_name.split()]))\n",
    "        self.korean_names = set(chain.from_iterable([unidecode(part).replace(\"-\",\" \").split() \n",
    "                                    for full_name in pd.read_csv(\"names_ko.txt\", header=None)[0].tolist() \n",
    "                                                     for part in full_name.split()]))\n",
    "        # vietnamese last names, cover 90% of population (wikipedia)\n",
    "        self.vietnamese_names = set(pd.read_csv(\"names_vn.txt\", header=None)[0].tolist())\n",
    "        self.ticketek_customers = pd.read_csv(\"ticketek_customers.csv.gz\", dtype=str).drop(\"middle_name\", axis=1)\n",
    "        self.ticketek_customers[\"full_name\"] = self.ticketek_customers[\"name\"] + \" \" + self.ticketek_customers[\"last_name\"]\n",
    "        # hypocorisms; we just make a set of these and don't care what names they relate to\n",
    "        self.hypoc_dict = json.load(open(\"hypocorisms.json\", \"r\")) \n",
    "        self.hypocs = {hyp for full_name in self.hypoc_dict\n",
    "                               for hyp in self.hypoc_dict[full_name]}\n",
    "        self.chinese_letter_stats = defaultdict()\n",
    "        self.chinese_cust_ids = set()\n",
    "        # a mask to exclude de longhi, te vroomer, etc that have parts like in chinese names\n",
    "        self.non_chinese_mask = re.compile('\\s+[ltodaelsn]{2,3}\\s+\\w{5,}(\\s|$)', re.ASCII)\n",
    "        # customer data frame will be processed by chunks specified below\n",
    "        self.CHUNK_SIZE = 100000    # in rows\n",
    "        self.FULL_CHUNKS, self.ROWS_LEFT = divmod(len(self.ticketek_customers), self.CHUNK_SIZE)\n",
    "    \n",
    "    def _last_n_letters(self, names, n):  \n",
    "        \n",
    "        last_letters = [w[-n:] for w in names if len(w[-n:]) == n]\n",
    "        most_frequent = statistics.mode(last_letters)\n",
    "              \n",
    "        return (set(last_letters), most_frequent)\n",
    "        \n",
    "    def get_name_stats(self):\n",
    "        \n",
    "        for nletters in range(1,4):\n",
    "            lst, top = self._last_n_letters(self.chinese_names, nletters)\n",
    "            self.chinese_letter_stats[\"last\" + str(nletters)] = lst\n",
    "            self.chinese_letter_stats[\"toplast\" + str(nletters)] = top\n",
    "        \n",
    "        self.chinese_letter_stats[\"max_len\"] = len(max(self.chinese_names, key=len))\n",
    "        self.chinese_letter_stats[\"avg_len\"] = round(statistics.mean([len(name) for name in self.chinese_names]),0)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _is_hypoc(self, st):\n",
    "        return 1 if (set(str(st).split()) & self.hypocs) else 0\n",
    "    \n",
    "    def _is_chinese(self, st):\n",
    "        return 1 if (set(str(st).split()) & self.chinese_names) else 0\n",
    "    \n",
    "    def _is_korean(self, st):\n",
    "        return 1 if (set(str(st).split()) & self.korean_names) else 0\n",
    "    \n",
    "    def _is_vietnamese(self, st):\n",
    "        return 1 if (set(str(st).split()) & self.vietnamese_names) else 0\n",
    "    \n",
    "    def _is_english_word(self, st):\n",
    "        return int(self.ench_dic.check(st))\n",
    "    \n",
    "    def is_likely_chinese(self, st):\n",
    "        if (self._is_chinese(st) and \n",
    "            (not self._is_korean(st)) and \n",
    "                (not self._is_hypoc(st)) and \n",
    "                    (all([not self._is_english_word(part) for part in str(st).split()]))):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def get_top_popular_last_names(self, n):\n",
    "        \n",
    "        self.most_popular_last_names = sorted([(k,v) for k, v in Counter(self.chinese_df[\"last_name\"].tolist()).items()], \n",
    "                                              key=lambda x: x[1], reverse=True)\n",
    "        return self\n",
    "        \n",
    "    def find_chinese_in_dataframe(self):\n",
    "        \n",
    "        print(\"searching for chinese among all Ticketek customers...\")\n",
    "\n",
    "        for i in range(self.FULL_CHUNKS + 1):\n",
    "            \n",
    "            LAST_ONE = i*self.CHUNK_SIZE + self.CHUNK_SIZE if i < self.FULL_CHUNKS else i*self.CHUNK_SIZE + self.ROWS_LEFT\n",
    "            sub_df = self.ticketek_customers.iloc[i*self.CHUNK_SIZE:LAST_ONE,:]\n",
    "            \n",
    "            self.chinese_cust_ids.update(set(sub_df.loc[sub_df[\"full_name\"].apply(self.is_likely_chinese) & \n",
    "                       sub_df[\"last_name\"].apply(lambda _: (sum([str(_).endswith(w) for w in self.chinese_letter_stats[\"last2\"]]) > 0)\n",
    "                                                and (not self.non_chinese_mask.search(str(_))) and\n",
    "                                                (not self._is_vietnamese(str(_)))),\n",
    "                       \"cust_id\"].tolist()))\n",
    "\n",
    "            if (i%20 == 0) or (i == self.FULL_CHUNKS-1):\n",
    "                print(\"ids found so far: {}\".format(len(self.chinese_cust_ids)))\n",
    "                \n",
    "        self.chinese_df = self.ticketek_customers.loc[self.ticketek_customers.cust_id.isin(self.chinese_cust_ids),:]\n",
    "        \n",
    "        return self\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    cf = ChineseFinder().get_name_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "searching for chinese among all Ticketek customers...\n",
      "ids found so far: 513\n",
      "ids found so far: 11686\n",
      "ids found so far: 19374\n",
      "ids found so far: 27016\n",
      "ids found so far: 38545\n",
      "ids found so far: 49869\n",
      "ids found so far: 63247\n",
      "ids found so far: 79877\n",
      "ids found so far: 96647\n",
      "ids found so far: 121552\n",
      "ids found so far: 146203\n",
      "ids found so far: 147323\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.ChineseFinder at 0x11fe7deb8>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf.find_chinese_in_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
