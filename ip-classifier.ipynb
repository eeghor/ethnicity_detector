{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from pprint import pprint\n",
    "from string import punctuation\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from unidecode import unidecode\n",
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EP:\n",
    "    \n",
    "    def __init__(self, ethnicity):  \n",
    "        \n",
    "        assert ethnicity in ('iranian polish italian german japanese vietnamese' \n",
    "                            ' portuguese fijian indian croatian greek serbian').split(), f'incorrect ethnicity: {ethnicity}!'\n",
    "        \n",
    "        datafile = f'/Users/ik/Data/names/training-{ethnicity}.csv'\n",
    "        self.target_col = f'is_{ethnicity}'\n",
    "        \n",
    "        self.data = pd.read_csv(datafile).sample(frac=1.)\n",
    "        self.data['full_name'] = self.data['full_name'].apply(lambda _: unidecode(_))\n",
    "        \n",
    "        assert self.target_col in self.data.columns, f'there is no {self.target_col} in data file!'  \n",
    "        \n",
    "        try:\n",
    "            self.NODOUBT_FIRST_NAMES = list({line.strip() for line in open(f'/Users/ik/Data/names/real_{ethnicity}.txt','r').readlines() if line.strip()})\n",
    "        except:\n",
    "            self.NODOUBT_FIRST_NAMES = []\n",
    "        \n",
    "    def add_nodoubt_names(self):\n",
    "        \n",
    "        print(f'adding {len(self.NODOUBT_FIRST_NAMES)} first names..')\n",
    "        \n",
    "        self.data = pd.concat([self.data, pd.DataFrame({'full_name': self.NODOUBT_FIRST_NAMES, \n",
    "                                                        self.target_col: [1]*len(self.NODOUBT_FIRST_NAMES)})], \n",
    "                              ignore_index=True).sample(frac=1.)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Selector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    select a columns from a data frame and return as a list\n",
    "    \"\"\"\n",
    "    def __init__(self, col_name):\n",
    "        self.col_name = col_name\n",
    "    \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, x):\n",
    "        return '_start_' + x[self.col_name] + '_end_'\n",
    "\n",
    "class WordCount(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    select a columns from a data frame and return as a list\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, x):\n",
    "        res = x.apply(lambda _: len(_.split())).values.reshape(x.shape[0],1)\n",
    "        return res\n",
    "\n",
    "class NameLength(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    return the length of the full name\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, x):\n",
    "        res = x.str.len().values.reshape(x.shape[0],1)\n",
    "        return res\n",
    "\n",
    "class FirstLast(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    is the first word longer than the last one\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, x):\n",
    "        res = x.apply(lambda _: np.argmax([len(p) for i, p in enumerate(_.split()) if i in [0,len(_.split())-1]])).values.reshape(x.shape[0],1)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    ic = EP('serbian')\n",
    "    #.add_nodoubt_names()\n",
    "    \n",
    "    # split into the training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(ic.data.drop([ic.target_col], axis=1), \n",
    "                                                        ic.data[ic.target_col],  test_size=0.25, \n",
    "                                                        random_state=391, stratify=ic.data[ic.target_col])\n",
    "    \n",
    "\n",
    "    pipe = Pipeline([('select_fullname', Selector('full_name')),  # df with 1 column\n",
    "                        ('features', FeatureUnion(\n",
    "                             [('char_level', CountVectorizer(strip_accents='ascii', analyzer='char', \n",
    "                                                          ngram_range=(1, 4))),\n",
    "                              ('word_level', CountVectorizer(strip_accents='ascii', analyzer='word',\n",
    "                                                       ngram_range=(1,2))),\n",
    "                              ('word_count', WordCount()),\n",
    "                              ('full_name_length', NameLength()),\n",
    "                              ('firstlast', FirstLast())],\n",
    "                                 transformer_weights={\n",
    "                                            'char_level': 0.8,\n",
    "                                            'word_level': 0.5,\n",
    "                                            'word_lengths': 1.0,\n",
    "                                                        })\n",
    "                     ),\n",
    "                     ('normalise', Normalizer()),\n",
    "#                      ('pca', TruncatedSVD(n_components=120)),\n",
    "                     ('clf', VotingClassifier(estimators=[('sgd', SGDClassifier(max_iter=1000)), \n",
    "                                                           ('rf', RandomForestClassifier()),\n",
    "                                                             ('svc', SVC(C=0.0001))], voting='hard'))\n",
    "#                     ('clf', SGDClassifier(max_iter=1000))\n",
    "                    ])\n",
    "    \n",
    "    param_grid = {\n",
    "#         'features__transformer_weights': [{'char_level': 0.2,\n",
    "#                                                      'word_level': 0.1,\n",
    "#                                                      'word_count':0.4,\n",
    "#                                                     'full_name_length': 0.5}, \n",
    "#                                                    {'char_level': 0.9,\n",
    "#                                                      'word_level': 0.2,\n",
    "#                                                      'word_lengths':0.9,\n",
    "#                                                    'full_name_length': 0.3},\n",
    "#                                                    {'char_level': 0.9,\n",
    "#                                                      'word_level': 0.3,\n",
    "#                                                      'word_count':0.2,\n",
    "#                                                    'full_name_length': 0.7}],\n",
    "                    \"clf__sgd__loss\": ['modified_huber'], # also 'hinge', 'perceptron', 'log'\n",
    "                     'clf__sgd__penalty': ['l2'],  # 'elasticnet', 'l1'\n",
    "                  'clf__rf__n_estimators': [100, 200],\n",
    "                  'clf__rf__max_depth': [None, 3],\n",
    "                    'clf__svc__C': [1e-5, 1e-4, 1e-2, 1]}  # default is 1.\n",
    "#                  'pca__n_components': [30, 60, 120, 180]}\n",
    "    \n",
    "    grid_search = GridSearchCV(pipe, param_grid=param_grid, verbose=10)\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, grid_search.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.DataFrame({'full_name': ['han solo', 'xi','babak', 'igor souza', 'yash patel', 'david perron',\n",
    "                                        'andrzej woods','otto floms', 'suzuki', 'toyota', \n",
    "                                        'edilson', 'kawasaki','silva', 'james tran', \n",
    "                                        'bo nguyen', 'subaru', 'ronaldo barbosa', 'ahmad lopes zeto', \n",
    "                                        'bob', 'mario razzi', 'john reed', 'andreas vlachos', 'con', 'nemanja vidic']})\n",
    "\n",
    "test_data['prediction'] = grid_search.predict(test_data)\n",
    "test_data['prediction'] = test_data['prediction'].apply(lambda _: 'yes' if _ else 'no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in zip(X_test.values, grid_search.predict(X_test), y_test.values):\n",
    "    if t[1] != t[2]:\n",
    "        print(f'{t[0][0].upper()} predicted {t[1]} but it is {t[2]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
